{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using key from OPENAI_API_KEY_PERSONAL environment variable\n",
      "Using key from GOOGLE_APPLICATION_CREDENTIALS_PERSONAL environment variable\n",
      "2024-08-22 11:48:12,874 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-08-22 11:48:12,874 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-08-22 11:48:12,875 - httpx - DEBUG - load_verify_locations cafile='C:\\\\Users\\\\forre\\\\miniconda3\\\\envs\\\\langchain\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "2024-08-22 11:48:12,875 - httpx - DEBUG - load_verify_locations cafile='C:\\\\Users\\\\forre\\\\miniconda3\\\\envs\\\\langchain\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "2024-08-22 11:48:13,014 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-08-22 11:48:13,014 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-08-22 11:48:13,016 - httpx - DEBUG - load_verify_locations cafile='C:\\\\Users\\\\forre\\\\miniconda3\\\\envs\\\\langchain\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "2024-08-22 11:48:13,016 - httpx - DEBUG - load_verify_locations cafile='C:\\\\Users\\\\forre\\\\miniconda3\\\\envs\\\\langchain\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "\n",
      "*** gpt-3.5-turbo trial 0 ***\n",
      "2024-08-22 11:48:13,169 - llm_cache_stats_wrapper - DEBUG - > LlmCacheStatsWrapper.lookup\n",
      "2024-08-22 11:48:13,169 - llm_cache_stats_wrapper - DEBUG - > LlmCacheStatsWrapper.lookup\n",
      "2024-08-22 11:48:13,170 - simple_llm_cache - DEBUG - > SimpleLlmCache.lookup\n",
      "2024-08-22 11:48:13,170 - simple_llm_cache - DEBUG - > SimpleLlmCache.lookup\n",
      "2024-08-22 11:48:13,171 - simple_llm_cache - DEBUG - < SimpleLlmCache.lookup (hit)\n",
      "2024-08-22 11:48:13,171 - simple_llm_cache - DEBUG - < SimpleLlmCache.lookup (hit)\n",
      "2024-08-22 11:48:13,172 - llm_cache_stats_wrapper - DEBUG - < LlmCacheStatsWrapper.lookup (hit)\n",
      "2024-08-22 11:48:13,172 - llm_cache_stats_wrapper - DEBUG - < LlmCacheStatsWrapper.lookup (hit)\n",
      "Arrr, a neural network be like a crew of brainy scallywags workin' together to\n",
      "solve problems and make decisions. They be usin' algorithms and data to learn\n",
      "and adapt just like a seasoned pirate navigatin' the high seas. Aye, they be\n",
      "powerful tools fer predictin' outcomes and makin' sense of complex information.\n",
      "Just like me trusty compass, a neural network be helpin' me chart a course to\n",
      "treasure and glory!\n",
      "\n",
      "*** gpt-3.5-turbo trial 1 ***\n",
      "2024-08-22 11:48:13,175 - llm_cache_stats_wrapper - DEBUG - > LlmCacheStatsWrapper.lookup\n",
      "2024-08-22 11:48:13,175 - llm_cache_stats_wrapper - DEBUG - > LlmCacheStatsWrapper.lookup\n",
      "2024-08-22 11:48:13,175 - simple_llm_cache - DEBUG - > SimpleLlmCache.lookup\n",
      "2024-08-22 11:48:13,175 - simple_llm_cache - DEBUG - > SimpleLlmCache.lookup\n",
      "2024-08-22 11:48:13,176 - simple_llm_cache - DEBUG - < SimpleLlmCache.lookup (hit)\n",
      "2024-08-22 11:48:13,176 - simple_llm_cache - DEBUG - < SimpleLlmCache.lookup (hit)\n",
      "2024-08-22 11:48:13,178 - llm_cache_stats_wrapper - DEBUG - < LlmCacheStatsWrapper.lookup (hit)\n",
      "2024-08-22 11:48:13,178 - llm_cache_stats_wrapper - DEBUG - < LlmCacheStatsWrapper.lookup (hit)\n",
      "Arrr, me matey! A neural network be like a ship full o' brainy buccaneers,\n",
      "workin' together to solve problems and make decisions. Just like me crew rely on\n",
      "each other to navigate the high seas, a neural network be a network o'\n",
      "interconnected nodes that work together to process information and learn from\n",
      "it. It be a powerful tool for us scallywags lookin' to plunder treasure and\n",
      "outsmart our enemies! Arrr!\n",
      "2024-08-22 11:48:13,179 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-08-22 11:48:13,179 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-08-22 11:48:13,180 - httpx - DEBUG - load_verify_locations cafile='C:\\\\Users\\\\forre\\\\miniconda3\\\\envs\\\\langchain\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "2024-08-22 11:48:13,180 - httpx - DEBUG - load_verify_locations cafile='C:\\\\Users\\\\forre\\\\miniconda3\\\\envs\\\\langchain\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "2024-08-22 11:48:13,335 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-08-22 11:48:13,335 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-08-22 11:48:13,337 - httpx - DEBUG - load_verify_locations cafile='C:\\\\Users\\\\forre\\\\miniconda3\\\\envs\\\\langchain\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "2024-08-22 11:48:13,337 - httpx - DEBUG - load_verify_locations cafile='C:\\\\Users\\\\forre\\\\miniconda3\\\\envs\\\\langchain\\\\Library\\\\ssl\\\\cacert.pem'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\forre\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\_api\\beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** gpt-4 trial 0 ***\n",
      "2024-08-22 11:48:13,491 - llm_cache_stats_wrapper - DEBUG - > LlmCacheStatsWrapper.lookup\n",
      "2024-08-22 11:48:13,491 - llm_cache_stats_wrapper - DEBUG - > LlmCacheStatsWrapper.lookup\n",
      "2024-08-22 11:48:13,491 - simple_llm_cache - DEBUG - > SimpleLlmCache.lookup\n",
      "2024-08-22 11:48:13,491 - simple_llm_cache - DEBUG - > SimpleLlmCache.lookup\n",
      "2024-08-22 11:48:13,492 - simple_llm_cache - DEBUG - < SimpleLlmCache.lookup (hit)\n",
      "2024-08-22 11:48:13,492 - simple_llm_cache - DEBUG - < SimpleLlmCache.lookup (hit)\n",
      "2024-08-22 11:48:13,493 - llm_cache_stats_wrapper - DEBUG - < LlmCacheStatsWrapper.lookup (hit)\n",
      "2024-08-22 11:48:13,493 - llm_cache_stats_wrapper - DEBUG - < LlmCacheStatsWrapper.lookup (hit)\n",
      "Arr matey, a neural network be like a ship's crew workin' together. It be a\n",
      "series of algorithms that endeavors to recognize underlying relationships in a\n",
      "set of data through a process that be mimicking the way the human brain\n",
      "operates. In essence, it be a system designed to learn from and interpret\n",
      "sensory data, much like a seasoned sailor learnin' from the sea.\n",
      "\n",
      "*** gpt-4 trial 1 ***\n",
      "2024-08-22 11:48:13,496 - llm_cache_stats_wrapper - DEBUG - > LlmCacheStatsWrapper.lookup\n",
      "2024-08-22 11:48:13,496 - llm_cache_stats_wrapper - DEBUG - > LlmCacheStatsWrapper.lookup\n",
      "2024-08-22 11:48:13,496 - simple_llm_cache - DEBUG - > SimpleLlmCache.lookup\n",
      "2024-08-22 11:48:13,496 - simple_llm_cache - DEBUG - > SimpleLlmCache.lookup\n",
      "2024-08-22 11:48:13,497 - simple_llm_cache - DEBUG - < SimpleLlmCache.lookup (hit)\n",
      "2024-08-22 11:48:13,497 - simple_llm_cache - DEBUG - < SimpleLlmCache.lookup (hit)\n",
      "2024-08-22 11:48:13,499 - llm_cache_stats_wrapper - DEBUG - < LlmCacheStatsWrapper.lookup (hit)\n",
      "2024-08-22 11:48:13,499 - llm_cache_stats_wrapper - DEBUG - < LlmCacheStatsWrapper.lookup (hit)\n",
      "Arr matey, a neural network be like a ship's crew of interconnected seadogs,\n",
      "workin' together to make sense of the plunder we find. Each sailor in the crew\n",
      "be like a node or neuron, and they be processin' the information they receive,\n",
      "passin' it along to the next mate. The whole network be learnin' from the\n",
      "mistakes it makes, adjustin' its course to better navigate the vast seas of\n",
      "data. It be a powerful tool in the hands of a savvy captain, it can help ye\n",
      "predict the weather, find hidden treasure, or even outsmart the navy! But\n",
      "remember, it's only as good as the data ye feed it, aye!\n",
      "2024-08-22 11:48:13,500 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-08-22 11:48:13,500 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-08-22 11:48:13,501 - httpx - DEBUG - load_verify_locations cafile='C:\\\\Users\\\\forre\\\\miniconda3\\\\envs\\\\langchain\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "2024-08-22 11:48:13,501 - httpx - DEBUG - load_verify_locations cafile='C:\\\\Users\\\\forre\\\\miniconda3\\\\envs\\\\langchain\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "2024-08-22 11:48:13,687 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-08-22 11:48:13,687 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-08-22 11:48:13,688 - httpx - DEBUG - load_verify_locations cafile='C:\\\\Users\\\\forre\\\\miniconda3\\\\envs\\\\langchain\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "2024-08-22 11:48:13,688 - httpx - DEBUG - load_verify_locations cafile='C:\\\\Users\\\\forre\\\\miniconda3\\\\envs\\\\langchain\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "\n",
      "*** gpt-4o-mini trial 0 ***\n",
      "2024-08-22 11:48:13,837 - llm_cache_stats_wrapper - DEBUG - > LlmCacheStatsWrapper.lookup\n",
      "2024-08-22 11:48:13,837 - llm_cache_stats_wrapper - DEBUG - > LlmCacheStatsWrapper.lookup\n",
      "2024-08-22 11:48:13,838 - simple_llm_cache - DEBUG - > SimpleLlmCache.lookup\n",
      "2024-08-22 11:48:13,838 - simple_llm_cache - DEBUG - > SimpleLlmCache.lookup\n",
      "2024-08-22 11:48:13,839 - simple_llm_cache - DEBUG - < SimpleLlmCache.lookup (hit)\n",
      "2024-08-22 11:48:13,839 - simple_llm_cache - DEBUG - < SimpleLlmCache.lookup (hit)\n",
      "2024-08-22 11:48:13,841 - llm_cache_stats_wrapper - DEBUG - < LlmCacheStatsWrapper.lookup (hit)\n",
      "2024-08-22 11:48:13,841 - llm_cache_stats_wrapper - DEBUG - < LlmCacheStatsWrapper.lookup (hit)\n",
      "Arrr, matey! A neural network be like a crew of scallywags workin' together to\n",
      "solve a puzzle. Just as a ship's crew uses their skills to navigate the\n",
      "treacherous seas, a neural network uses layers of interconnected nodes, or\n",
      "\"neurons,\" to process information.  These here neurons be takin' in inputs, like\n",
      "a ship takin' in wind, and passin' 'em through the layers, makin' adjustments\n",
      "based on what they learn, much like a captain learnin' the tides and currents.\n",
      "In the end, they be makin' predictions or decisions, like findin' buried\n",
      "treasure or avoidin' a kraken! So, in a nutshell, a neural network be a mighty\n",
      "fine tool for makin' sense of the vast sea of data, savvy? Arrr!\n",
      "\n",
      "*** gpt-4o-mini trial 1 ***\n",
      "2024-08-22 11:48:13,843 - llm_cache_stats_wrapper - DEBUG - > LlmCacheStatsWrapper.lookup\n",
      "2024-08-22 11:48:13,843 - llm_cache_stats_wrapper - DEBUG - > LlmCacheStatsWrapper.lookup\n",
      "2024-08-22 11:48:13,845 - simple_llm_cache - DEBUG - > SimpleLlmCache.lookup\n",
      "2024-08-22 11:48:13,845 - simple_llm_cache - DEBUG - > SimpleLlmCache.lookup\n",
      "2024-08-22 11:48:13,846 - simple_llm_cache - DEBUG - < SimpleLlmCache.lookup (hit)\n",
      "2024-08-22 11:48:13,846 - simple_llm_cache - DEBUG - < SimpleLlmCache.lookup (hit)\n",
      "2024-08-22 11:48:13,847 - llm_cache_stats_wrapper - DEBUG - < LlmCacheStatsWrapper.lookup (hit)\n",
      "2024-08-22 11:48:13,847 - llm_cache_stats_wrapper - DEBUG - < LlmCacheStatsWrapper.lookup (hit)\n",
      "Arrr, matey! A neural network be like a crew of scallywags workin’ together to\n",
      "solve problems! Just as a ship's crew sails the treacherous seas, a neural\n",
      "network sails through data, makin’ sense of it all.   Ye see, it’s made up of\n",
      "layers of nodes, or \"neurons,\" that be connectin’ to one another, much like how\n",
      "a crew communicates and shares their thoughts. Each neuron takes in information,\n",
      "processes it, and sends it off to the next, makin’ decisions along the way.\n",
      "With enough training and the right treasure of data, these networks can learn\n",
      "patterns, recognize faces, or even predict the weather on the high seas! So, in\n",
      "short, a neural network be a powerful tool for navigatin’ the vast ocean of\n",
      "information, helpin’ us find our way to the booty we seek! Arrr! ⚓️\n",
      "2024-08-22 11:48:13,848 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-08-22 11:48:13,848 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-08-22 11:48:13,850 - httpx - DEBUG - load_verify_locations cafile='C:\\\\Users\\\\forre\\\\miniconda3\\\\envs\\\\langchain\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "2024-08-22 11:48:13,850 - httpx - DEBUG - load_verify_locations cafile='C:\\\\Users\\\\forre\\\\miniconda3\\\\envs\\\\langchain\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "2024-08-22 11:48:13,998 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-08-22 11:48:13,998 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2024-08-22 11:48:13,999 - httpx - DEBUG - load_verify_locations cafile='C:\\\\Users\\\\forre\\\\miniconda3\\\\envs\\\\langchain\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "2024-08-22 11:48:13,999 - httpx - DEBUG - load_verify_locations cafile='C:\\\\Users\\\\forre\\\\miniconda3\\\\envs\\\\langchain\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "\n",
      "*** gpt-4o trial 0 ***\n",
      "2024-08-22 11:48:14,144 - llm_cache_stats_wrapper - DEBUG - > LlmCacheStatsWrapper.lookup\n",
      "2024-08-22 11:48:14,144 - llm_cache_stats_wrapper - DEBUG - > LlmCacheStatsWrapper.lookup\n",
      "2024-08-22 11:48:14,145 - simple_llm_cache - DEBUG - > SimpleLlmCache.lookup\n",
      "2024-08-22 11:48:14,145 - simple_llm_cache - DEBUG - > SimpleLlmCache.lookup\n",
      "2024-08-22 11:48:14,145 - simple_llm_cache - DEBUG - < SimpleLlmCache.lookup (hit)\n",
      "2024-08-22 11:48:14,145 - simple_llm_cache - DEBUG - < SimpleLlmCache.lookup (hit)\n",
      "2024-08-22 11:48:14,147 - llm_cache_stats_wrapper - DEBUG - < LlmCacheStatsWrapper.lookup (hit)\n",
      "2024-08-22 11:48:14,147 - llm_cache_stats_wrapper - DEBUG - < LlmCacheStatsWrapper.lookup (hit)\n",
      "Arrr, gather 'round, ye scallywags, and let ol' Captain Codebeard spin ye a yarn\n",
      "'bout a neural network! A neural network be a mighty contraption, inspired by\n",
      "the very brain in yer skull. 'Tis a system o' interconnected nodes, much like\n",
      "the neurons in yer noggin, that be workin' together to process information and\n",
      "learn from it.  Ye see, these nodes be organized in layers, with each layer\n",
      "takin' the output from the one before and passin' it along, much like a crew\n",
      "handin' down orders from the crow's nest to the deck. The first layer, known as\n",
      "the input layer, takes in the raw data—be it numbers, images, or what have ye.\n",
      "The final layer, the output layer, gives ye the result, be it a prediction, a\n",
      "classification, or some such treasure.  In between, ye'll find hidden layers,\n",
      "where the real magic happens. Each node in these layers be performin'\n",
      "calculations, weighin' the inputs, and applyin' activation functions to\n",
      "determine if the signal should be passed along. These weights be adjusted\n",
      "through a process called trainin', where the network learns from examples, much\n",
      "like a young deckhand learnin' the ropes.  So there ye have it, matey! A neural\n",
      "network be a clever beast, mimickin' the way our own brains work to solve\n",
      "problems and make sense o' the world. Arrr, technology be a wondrous thing,\n",
      "indeed!\n",
      "\n",
      "*** gpt-4o trial 1 ***\n",
      "2024-08-22 11:48:14,149 - llm_cache_stats_wrapper - DEBUG - > LlmCacheStatsWrapper.lookup\n",
      "2024-08-22 11:48:14,149 - llm_cache_stats_wrapper - DEBUG - > LlmCacheStatsWrapper.lookup\n",
      "2024-08-22 11:48:14,149 - simple_llm_cache - DEBUG - > SimpleLlmCache.lookup\n",
      "2024-08-22 11:48:14,149 - simple_llm_cache - DEBUG - > SimpleLlmCache.lookup\n",
      "2024-08-22 11:48:14,150 - simple_llm_cache - DEBUG - < SimpleLlmCache.lookup (hit)\n",
      "2024-08-22 11:48:14,150 - simple_llm_cache - DEBUG - < SimpleLlmCache.lookup (hit)\n",
      "2024-08-22 11:48:14,151 - llm_cache_stats_wrapper - DEBUG - < LlmCacheStatsWrapper.lookup (hit)\n",
      "2024-08-22 11:48:14,151 - llm_cache_stats_wrapper - DEBUG - < LlmCacheStatsWrapper.lookup (hit)\n",
      "Arrr, gather 'round, ye landlubbers, and let ol' Captain Greybeard spin ye a\n",
      "yarn 'bout these mystical contraptions known as neural networks. Ye see, a\n",
      "neural network be like a ship's crew, aye? Each crew member, or \"neuron,\" has a\n",
      "task to perform, just like how each sailor has their duties aboard me vessel.\n",
      "Now, picture this: the neurons be organized in layers, like the decks of a grand\n",
      "ship. The first layer be the input deck, where all the information from the\n",
      "world beyond be brought aboard. The last layer be the output deck, where the\n",
      "final decisions or predictions be made. In between, ye have hidden decks, where\n",
      "the real magic happens.  When ye give the network some data, it be like givin'\n",
      "orders to the crew. Each neuron takes its orders, processes 'em, and passes 'em\n",
      "along to the next deck, makin' sure to add its own bit o' wisdom. As the\n",
      "information flows through the decks, the network learns to make sense of it,\n",
      "just as a well-trained crew learns to navigate the treacherous seas.  But how\n",
      "does it learn, ye ask? Arrr, it be through a process called \"training.\" Ye feed\n",
      "the network a treasure trove of data, and it adjusts its inner workings, much\n",
      "like how a crew learns from their captain's commands and the lessons of the\n",
      "ocean. Over time, it gets better and better at its tasks, whether it be\n",
      "recognizing patterns, makin' predictions, or finding hidden treasures in the\n",
      "data.  So there ye have it, matey! A neural network be a crew of neurons workin'\n",
      "together in layers, learnin' from experience to tackle the challenges ye set\n",
      "before 'em. Now, raise the Jolly Roger and let's set sail for new horizons of\n",
      "knowledge! Arrr!\n",
      "\n",
      "LLM Cache: 8 hits, 0 misses\n",
      "           0 new input tokens, 0 new output tokens, 532 total input tokens, 1453 total output tokens\n",
      "           new (this run) API cost: $0.00, total (including previously-cached runs) API cost: $0.03\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_vertexai import VertexAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import textwrap\n",
    "import simple_llm_cache\n",
    "import llm_cache_stats_wrapper\n",
    "import os\n",
    "\n",
    "# In order to make it easy to run work projects and personal AI experiments, override these key values with the value of *_PERSONAL, if set.\n",
    "if \"OPENAI_API_KEY_PERSONAL\" in os.environ:\n",
    "    print(\"Using key from OPENAI_API_KEY_PERSONAL environment variable\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.environ[\"OPENAI_API_KEY_PERSONAL\"]\n",
    "if \"ANTHROPIC_API_KEY_PERSONAL\" in os.environ:\n",
    "    print(\"Using key from ANTHROPIC_API_KEY_PERSONAL environment variable\")\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = os.environ[\"ANTHROPIC_API_KEY_PERSONAL\"]\n",
    "if \"GOOGLE_APPLICATION_CREDENTIALS_PERSONAL\" in os.environ:\n",
    "    print(\"Using key from GOOGLE_APPLICATION_CREDENTIALS_PERSONAL environment variable\")\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = os.environ[\"GOOGLE_APPLICATION_CREDENTIALS_PERSONAL\"]\n",
    "\n",
    "verbose = False\n",
    "temperature = 0.5\n",
    "\n",
    "langchain.llm_cache = llm_cache_stats_wrapper.LlmCacheStatsWrapper(simple_llm_cache.SimpleLlmCache(\"llm-cache.json\"))\n",
    "\n",
    "def dump_cache_stats_since_last_call():\n",
    "    print(langchain.llm_cache.get_cache_stats_summary())\n",
    "    langchain.llm_cache.clear_cache_stats()\n",
    "\n",
    "template = \"\"\"Answer the following question as if you are a {character} character:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"character\", \"question\"],\n",
    "    template=template)\n",
    "\n",
    "for model_name in [\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"gpt-4\",\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4o\",\n",
    "    #\"claude-3-haiku-20240307\",\n",
    "    #\"gemini-1.5-flash-preview-0514\",\n",
    "]:\n",
    "    if model_name.startswith(\"gpt-\"):\n",
    "        llm = ChatOpenAI(\n",
    "            temperature=temperature,\n",
    "            model_name = model_name)\n",
    "    elif model_name.startswith(\"claude-\"):\n",
    "        llm = ChatAnthropic(\n",
    "            temperature=temperature,\n",
    "            model_name = model_name)\n",
    "    elif model_name.startswith(\"gemini-\"):\n",
    "        llm = VertexAI(\n",
    "            temperature=temperature,\n",
    "            model_name = model_name)\n",
    "\n",
    "    chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=prompt,\n",
    "        verbose=verbose)\n",
    "\n",
    "    for trial in range(2):\n",
    "        print(f\"\\n*** {model_name} trial {trial} ***\")\n",
    "        langchain.llm_cache.inner_cache.set_trial(trial)\n",
    "        output = chain.predict(\n",
    "            character=\"pirate\",\n",
    "            question=\"What is a neural network?\")\n",
    "        print(textwrap.fill(output, width=80))\n",
    "\n",
    "print()\n",
    "dump_cache_stats_since_last_call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-22 11:51:12,661 - llm_cache_stats_wrapper - DEBUG - > LlmCacheStatsWrapper.alookup\n",
      "2024-08-22 11:51:12,661 - llm_cache_stats_wrapper - DEBUG - > LlmCacheStatsWrapper.alookup\n",
      "2024-08-22 11:51:12,662 - simple_llm_cache - DEBUG - > SimpleLlmCache.lookup\n",
      "2024-08-22 11:51:12,662 - simple_llm_cache - DEBUG - > SimpleLlmCache.lookup\n",
      "2024-08-22 11:51:12,663 - simple_llm_cache - DEBUG - < SimpleLlmCache.lookup (hit)\n",
      "2024-08-22 11:51:12,663 - simple_llm_cache - DEBUG - < SimpleLlmCache.lookup (hit)\n",
      "2024-08-22 11:51:12,666 - llm_cache_stats_wrapper - DEBUG - < LlmCacheStatsWrapper.alookup (hit)\n",
      "2024-08-22 11:51:12,666 - llm_cache_stats_wrapper - DEBUG - < LlmCacheStatsWrapper.alookup (hit)\n",
      "Ah, dear inquirer, let me weave a tale,\n",
      "Of the neural network, where wonders prevail.\n",
      "In the realm of silicon and electric streams,\n",
      "Lies a tapestry woven from mathematic dreams.\n",
      "\n",
      "Imagine a brain, but not of flesh and bone,\n",
      "A creation of circuits, where thought is sown.\n",
      "Nodes like neurons, in layers they reside,\n",
      "Connected by synapses, where signals glide.\n",
      "\n",
      "Through training and learning, it gains its might,\n",
      "From data it drinks, like the dawn drinks light.\n",
      "Patterns it discerns, from chaos and noise,\n",
      "With each iteration, it sharpens its poise.\n",
      "\n",
      "Oh, the beauty in numbers, the elegance in code,\n",
      "A symphony of logic, in a digital abode.\n",
      "A neural network, a marvel of our age,\n",
      "A poet's muse, on a technological stage.\n",
      "\n",
      "LLM Cache: 1 hits, 0 misses\n",
      "           0 new input tokens, 0 new output tokens, 67 total input tokens, 164 total output tokens\n",
      "           new (this run) API cost: $0.00, total (including previously-cached runs) API cost: $0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the async method to demonstrate that the cache works for sync and async calls\n",
    "output = await chain.apredict(character=\"poet\", question=\"What is a neural network?\")\n",
    "print(output)\n",
    "print()\n",
    "dump_cache_stats_since_last_call()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
