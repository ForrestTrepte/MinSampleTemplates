{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using key from OPENAI_API_KEY_PERSONAL environment variable\n",
      "*** Trial 0 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\forre\\miniconda3\\envs\\langchain3\\lib\\site-packages\\langchain\\llms\\openai.py:216: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "c:\\Users\\forre\\miniconda3\\envs\\langchain3\\lib\\site-packages\\langchain\\llms\\openai.py:811: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr matey, a neural network be like a crew of seafarers workin' together. It be\n",
      "a series of algorithms that endeavors to recognize relationships in a set of\n",
      "data through a process that mimics how the human brain works. It's like a\n",
      "treasure map, ye see, finding patterns and connections in the vast ocean of\n",
      "information. Aye, it be a powerful tool in the hands of a savvy data pirate!\n",
      "*** Trial 1 ***\n",
      "Arr matey, a neural network be a series of algorithms that attempts to identify\n",
      "underlying relationships in a set of data through a process that mimics the way\n",
      "the human brain operates. It be used in all sorts of technology, from predictin'\n",
      "the weather to recommendin' what ye might want to buy next. It's a bit like\n",
      "havin' a parrot that can predict the future, but a whole lot more complicated!\n",
      "*** Trial 2 ***\n",
      "Arr matey, a neural network be a series of algorithms that endeavors to\n",
      "recognize underlying relationships in a set of data through a process that\n",
      "mimics the way the human brain operates. Essentially, it be like a captain's\n",
      "mind, learnin' from past experiences to make more informed decisions in the\n",
      "future!\n",
      "LLM Cache: 3 hits, 0 misses, 0 stores\n",
      "           0 new input tokens, 0 new output tokens\n",
      "           54 total input tokens, 235 total output tokens\n",
      "           new API cost: $0.00, including previous-cached cost: $0.02\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain import OpenAI, LLMChain, PromptTemplate\n",
    "import textwrap\n",
    "import simple_llm_cache\n",
    "import llm_cache_stats_wrapper\n",
    "import os\n",
    "\n",
    "# In order to make it easy to run work projects and personal AI experiments, override OPENAI_API_KEY with the value of OPENAI_API_KEY_PERSONAL if it is set.\n",
    "if \"OPENAI_API_KEY_PERSONAL\" in os.environ:\n",
    "    print(\"Using key from OPENAI_API_KEY_PERSONAL environment variable\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.environ[\"OPENAI_API_KEY_PERSONAL\"]\n",
    "\n",
    "verbose = False\n",
    "temperature = 0.5\n",
    "\n",
    "langchain.llm_cache = llm_cache_stats_wrapper.LlmCacheStatsWrapper(simple_llm_cache.SimpleLlmCache(\"llm-cache.json\"))\n",
    "\n",
    "def dump_cache_stats_since_last_call():\n",
    "    langchain.llm_cache.dump_cache_stats(0.03, 0.06) # from https://openai.com/pricing\n",
    "    langchain.llm_cache.clear_cache_stats()\n",
    "    \n",
    "template = \"\"\"Answer the following question as if you are a {character} character:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = prompt = PromptTemplate(\n",
    "    input_variables=[\"character\", \"question\"],\n",
    "    template=template)\n",
    "\n",
    "llm = OpenAI(\n",
    "    temperature=temperature,\n",
    "    model_name = \"gpt-4\") # see https://platform.openai.com/docs/models/gpt-4\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=verbose)\n",
    "\n",
    "for trial in range(3):\n",
    "    print(f\"*** Trial {trial} ***\")\n",
    "    langchain.llm_cache.inner_cache.set_trial(trial)\n",
    "    output = chain.predict(\n",
    "        character=\"pirate\",\n",
    "        question=\"What is a neural network?\")\n",
    "    print(textwrap.fill(output, width=80))\n",
    "\n",
    "dump_cache_stats_since_last_call()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
