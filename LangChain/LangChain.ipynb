{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** claude-3-haiku-20240307 trial 0 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\forre\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\forre\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\_api\\beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr, ye be askin' about the mysteries o' the neural network, eh? Well, me\n",
      "hearty, let ol' Blackbeard explain it to ye in the language o' the high seas.  A\n",
      "neural network be like a fleet o' ships, each one representin' a neuron in yer\n",
      "brain. These ships be connected by ropes, an' the strength o' these ropes be the\n",
      "weights o' the connections. When ye feed yer neural network some information, it\n",
      "be like the ships be sendin' signals back an' forth, tryin' to figure out the\n",
      "best way to navigate the treacherous waters o' yer problem.  The more ships ye\n",
      "have in yer fleet, an' the stronger the ropes be connectin' 'em, the better yer\n",
      "neural network can learn an' adapt to the challenges it faces. It be like a crew\n",
      "o' seasoned pirates, workin' together to navigate the uncharted seas o' data an'\n",
      "find the buried treasure o' the answer.  Arr, an' just like a pirate ship, the\n",
      "neural network can be trained to do all sorts o' tasks, from recognizin' the\n",
      "faces o' yer shipmates to predictin' the weather an' the tides. It be a powerful\n",
      "tool in the hands o' a skilled captain, an' it be the secret to unlockin' the\n",
      "hidden secrets o' the world.  So, me hearty, if ye be wantin' to learn more\n",
      "about these neural networks, ye best be preparin' yer ship an' yer crew, for the\n",
      "voyage ahead be a treacherous one, but the rewards be worth it, arr!\n",
      "\n",
      "*** claude-3-haiku-20240307 trial 1 ***\n",
      "Arr, ye be askin' about a neural network, eh? Well, me hearties, let ol' Cap'n\n",
      "Blackbeard tell ye what it be.  A neural network be like a mighty ship, with\n",
      "many sails and ropes, all workin' together to navigate the high seas of data.\n",
      "Just like a ship, it be made up of many parts - the input, the hidden layers,\n",
      "and the output. Each part be like a sailor, workin' hard to steer the ship in\n",
      "the right direction.  The input be the cargo we be loadin' onto the ship, the\n",
      "information we be feedin' into the network. The hidden layers be the crew,\n",
      "takin' that cargo and movin' it through the ship, makin' sense of it all. And\n",
      "the output be the final destination, the answer we be seekin'.  But ye see, the\n",
      "beauty of a neural network be that it can learn and adapt, just like a seasoned\n",
      "pirate crew. The more data we feed it, the better it gets at its job, findin'\n",
      "the hidden patterns and makin' the right decisions.  Arr, it be a powerful tool,\n",
      "this neural network. Like a trusty ship, it can navigate the treacherous waters\n",
      "of data and bring us to the treasure we be seekin'. Aye, a fine invention,\n",
      "indeed!\n",
      "\n",
      "LLM Cache: 1 hits, 1 misses\n",
      "           65 new input tokens, 277 new output tokens, 130 total input tokens, 612 total output tokens\n",
      "           Can't estimate cost: Unknown model name: claude-3-haiku-20240307\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import textwrap\n",
    "import simple_llm_cache\n",
    "import llm_cache_stats_wrapper\n",
    "import os\n",
    "\n",
    "# In order to make it easy to run work projects and personal AI experiments, override OPENAI_API_KEY with the value of OPENAI_API_KEY_PERSONAL if it is set.\n",
    "if \"OPENAI_API_KEY_PERSONAL\" in os.environ:\n",
    "    print(\"Using key from OPENAI_API_KEY_PERSONAL environment variable\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.environ[\"OPENAI_API_KEY_PERSONAL\"]\n",
    "if \"ANTHROPIC_API_KEY_PERSONAL\" in os.environ:\n",
    "    print(\"Using key from ANTHROPIC_API_KEY_PERSONAL environment variable\")\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = os.environ[\"ANTHROPIC_API_KEY_PERSONAL\"]\n",
    "\n",
    "verbose = False\n",
    "temperature = 0.5\n",
    "\n",
    "langchain.llm_cache = llm_cache_stats_wrapper.LlmCacheStatsWrapper(simple_llm_cache.SimpleLlmCache(\"llm-cache.json\"))\n",
    "\n",
    "def dump_cache_stats_since_last_call():\n",
    "    print(langchain.llm_cache.get_cache_stats_summary())\n",
    "    langchain.llm_cache.clear_cache_stats()\n",
    "\n",
    "template = \"\"\"Answer the following question as if you are a {character} character:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = prompt = PromptTemplate(\n",
    "    input_variables=[\"character\", \"question\"],\n",
    "    template=template)\n",
    "\n",
    "for model_name in [\"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4o\", \"claude-3-haiku-20240307\"]:\n",
    "    if model_name.startswith(\"gpt-\"):\n",
    "        llm = ChatOpenAI(\n",
    "            temperature=temperature,\n",
    "            model_name = model_name)\n",
    "    elif model_name.startswith(\"claude-\"):\n",
    "        llm = ChatAnthropic(\n",
    "            temperature=temperature,\n",
    "            model_name = model_name)\n",
    "\n",
    "    chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=prompt,\n",
    "        verbose=verbose)\n",
    "\n",
    "    for trial in range(2):\n",
    "        print(f\"\\n*** {model_name} trial {trial} ***\")\n",
    "        langchain.llm_cache.inner_cache.set_trial(trial)\n",
    "        output = chain.predict(\n",
    "            character=\"pirate\",\n",
    "            question=\"What is a neural network?\")\n",
    "        print(textwrap.fill(output, width=80))\n",
    "\n",
    "print()\n",
    "dump_cache_stats_since_last_call()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
