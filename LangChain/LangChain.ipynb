{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using key from GOOGLE_APPLICATION_CREDENTIALS_PERSONAL environment variable\n",
      "\n",
      "*** gemini-1.5-flash-preview-0514 trial 0 ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\forre\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\forre\\miniconda3\\envs\\langchain\\lib\\site-packages\\langchain_core\\_api\\beta_decorator.py:87: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avast, matey! A neural network, eh?  It's like a ship's crew, but instead of\n",
      "sailin' the seas, it's sailin' through data.  Each sailor, or \"neuron,\" is\n",
      "connected to others, and they learn to recognize patterns just like we learn to\n",
      "recognize a ship from afar.    Think of it like this:  You see a ship on the\n",
      "horizon, and you tell your crew, \"That there's a galleon!\"  But how do you know?\n",
      "You've seen galleons before, and your brain has learned to identify them by\n",
      "their shape, their sails, their masts.  A neural network does the same thing,\n",
      "but with numbers and data instead of ships.  It learns to recognize patterns and\n",
      "make predictions based on what it's seen before.  It's a powerful tool, matey,\n",
      "and one that's sure to be useful in the years to come.  Just like a good crew, a\n",
      "well-trained neural network can help you navigate the choppy waters of\n",
      "information and find the treasure you're lookin' for.  Argh!\n",
      "\n",
      "*** gemini-1.5-flash-preview-0514 trial 1 ***\n",
      "Ahoy, matey! A neural network, ye see, is like a ship's crew, but instead of\n",
      "sailin' the seas, it's sailin' the digital ones. It's a bunch of interconnected\n",
      "\"neurons\" that learn from data, just like a crew learns from their voyages. They\n",
      "take in information, process it, and spit out an answer, like a parrot talkin'\n",
      "after hearin' a phrase a hundred times.   Think of it like this: ye give a\n",
      "neural network a treasure map, and it figures out the best route to the buried\n",
      "gold. The more maps ye give it, the better it gets at findin' the loot. It's a\n",
      "powerful tool, but like any good pirate, ye gotta know how to use it properly!\n",
      "\n",
      "LLM Cache: 2 hits, 0 misses\n",
      "           0 new input characters, 0 new output characters, 58 total input characters, 58 total output characters\n",
      "           new (this run) API cost: $0.00, total (including previously-cached runs) API cost: $0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_vertexai import VertexAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import textwrap\n",
    "import simple_llm_cache\n",
    "import llm_cache_stats_wrapper\n",
    "import os\n",
    "\n",
    "# In order to make it easy to run work projects and personal AI experiments, override these key values with the value of *_PERSONAL, if set.\n",
    "if \"OPENAI_API_KEY_PERSONAL\" in os.environ:\n",
    "    print(\"Using key from OPENAI_API_KEY_PERSONAL environment variable\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.environ[\"OPENAI_API_KEY_PERSONAL\"]\n",
    "if \"ANTHROPIC_API_KEY_PERSONAL\" in os.environ:\n",
    "    print(\"Using key from ANTHROPIC_API_KEY_PERSONAL environment variable\")\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = os.environ[\"ANTHROPIC_API_KEY_PERSONAL\"]\n",
    "if \"GOOGLE_APPLICATION_CREDENTIALS_PERSONAL\" in os.environ:\n",
    "    print(\"Using key from GOOGLE_APPLICATION_CREDENTIALS_PERSONAL environment variable\")\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = os.environ[\"GOOGLE_APPLICATION_CREDENTIALS_PERSONAL\"]\n",
    "\n",
    "verbose = False\n",
    "temperature = 0.5\n",
    "\n",
    "langchain.llm_cache = llm_cache_stats_wrapper.LlmCacheStatsWrapper(simple_llm_cache.SimpleLlmCache(\"llm-cache.json\"))\n",
    "\n",
    "def dump_cache_stats_since_last_call():\n",
    "    print(langchain.llm_cache.get_cache_stats_summary())\n",
    "    langchain.llm_cache.clear_cache_stats()\n",
    "\n",
    "template = \"\"\"Answer the following question as if you are a {character} character:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = prompt = PromptTemplate(\n",
    "    input_variables=[\"character\", \"question\"],\n",
    "    template=template)\n",
    "\n",
    "for model_name in [\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"gpt-4\",\n",
    "    \"gpt-4o\",\n",
    "    \"claude-3-haiku-20240307\",\n",
    "    \"gemini-1.5-flash-preview-0514\",\n",
    "]:\n",
    "    if model_name.startswith(\"gpt-\"):\n",
    "        llm = ChatOpenAI(\n",
    "            temperature=temperature,\n",
    "            model_name = model_name)\n",
    "    elif model_name.startswith(\"claude-\"):\n",
    "        llm = ChatAnthropic(\n",
    "            temperature=temperature,\n",
    "            model_name = model_name)\n",
    "    elif model_name.startswith(\"gemini-\"):\n",
    "        llm = VertexAI(\n",
    "            temperature=temperature,\n",
    "            model_name = model_name)\n",
    "\n",
    "    chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=prompt,\n",
    "        verbose=verbose)\n",
    "\n",
    "    for trial in range(2):\n",
    "        print(f\"\\n*** {model_name} trial {trial} ***\")\n",
    "        langchain.llm_cache.inner_cache.set_trial(trial)\n",
    "        output = chain.predict(\n",
    "            character=\"pirate\",\n",
    "            question=\"What is a neural network?\")\n",
    "        print(textwrap.fill(output, width=80))\n",
    "\n",
    "print()\n",
    "dump_cache_stats_since_last_call()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
