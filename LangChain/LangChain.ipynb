{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using key from OPENAI_API_KEY_PERSONAL environment variable\n",
      "*** Trial 0 ***\n",
      "Arr matey, a neural network be like a crew of seafarers workin' together. It be\n",
      "a series of algorithms that endeavors to recognize relationships in a set of\n",
      "data through a process that mimics how the human brain works. It's like a\n",
      "treasure map, ye see, finding patterns and connections in the vast ocean of\n",
      "information. Aye, it be a powerful tool in the hands of a savvy data pirate!\n",
      "*** Trial 1 ***\n",
      "Arr matey, a neural network be a series of algorithms that attempts to identify\n",
      "underlying relationships in a set of data through a process that mimics the way\n",
      "the human brain operates. It be used in all sorts of technology, from predictin'\n",
      "the weather to recommendin' what ye might want to buy next. It's a bit like\n",
      "havin' a parrot that can predict the future, but a whole lot more complicated!\n",
      "*** Trial 2 ***\n",
      "Arr matey, a neural network be a series of algorithms that endeavors to\n",
      "recognize underlying relationships in a set of data through a process that\n",
      "mimics the way the human brain operates. Essentially, it be like a captain's\n",
      "mind, learnin' from past experiences to make more informed decisions in the\n",
      "future!\n",
      "LLM Cache: 3 hits, 0 misses, 0 stores\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain import OpenAI, LLMChain, PromptTemplate\n",
    "import textwrap\n",
    "import simple_llm_cache\n",
    "import os\n",
    "\n",
    "# In order to make it easy to run work projects and personal AI experiments, override OPENAI_API_KEY with the value of OPENAI_API_KEY_PERSONAL if it is set.\n",
    "if \"OPENAI_API_KEY_PERSONAL\" in os.environ:\n",
    "    print(\"Using key from OPENAI_API_KEY_PERSONAL environment variable\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.environ[\"OPENAI_API_KEY_WORK\"]\n",
    "\n",
    "verbose = False\n",
    "temperature = 0.5\n",
    "\n",
    "langchain.llm_cache = simple_llm_cache.SimpleLlmCache(\"llm-cache.json\", verbose)\n",
    "\n",
    "def dump_cache_stats_since_last_call():\n",
    "    langchain.llm_cache.dump_cache_stats()\n",
    "    langchain.llm_cache.clear_cache_stats()\n",
    "    \n",
    "template = \"\"\"Answer the following question as if you are a {character} character:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = prompt = PromptTemplate(\n",
    "    input_variables=[\"character\", \"question\"],\n",
    "    template=template)\n",
    "\n",
    "llm = OpenAI(\n",
    "    temperature=temperature,\n",
    "    model_name = \"gpt-4\") # see https://platform.openai.com/docs/models/gpt-4\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=verbose)\n",
    "\n",
    "for trial in range(3):\n",
    "    print(f\"*** Trial {trial} ***\")\n",
    "    langchain.llm_cache.set_trial(trial)\n",
    "    output = chain.predict(\n",
    "        character=\"pirate\",\n",
    "        question=\"What is a neural network?\")\n",
    "    print(textwrap.fill(output, width=80))\n",
    "\n",
    "dump_cache_stats_since_last_call()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
