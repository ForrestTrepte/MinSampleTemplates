{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using key from OPENAI_API_KEY_PERSONAL environment variable\n",
      "Using key from GOOGLE_APPLICATION_CREDENTIALS_PERSONAL environment variable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\forre\\AppData\\Local\\Temp\\ipykernel_30088\\1443925633.py:90: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n",
      "c:\\s\\MinSampleTemplates\\LangChain\\simple_llm_cache.py:116: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  generations.append(loads(generation))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** gemini-1.5-pro-001 trial 0 ***\n",
      "TRIAL 0\n",
      "[{\"capital\": \"Ottawa \\\"Bytown\\\"\", \"country\": \"Canada\"}, {\"capital\": \"Mexico City\n",
      "\\\"The City of Palaces\\\"\", \"country\": \"Mexico\"}, {\"capital\": \"Washington, D.C.\n",
      "\\\"The District\\\"\", \"country\": \"United States\"}]\n",
      "\n",
      "From JSON:\n",
      "Canada: Ottawa \"Bytown\"\n",
      "Mexico: Mexico City \"The City of Palaces\"\n",
      "United States: Washington, D.C. \"The District\"\n",
      "\n",
      "*** gemini-1.5-pro-001 trial 1 ***\n",
      "TRIAL 1\n",
      "[{\"capital\": \"Ottawa \\\"Bytown\\\"\", \"country\": \"Canada\"}, {\"capital\": \"Mexico City\n",
      "\\\"The City of Palaces\\\"\", \"country\": \"Mexico\"}, {\"capital\": \"Washington, D.C.\n",
      "\\\"The District\\\"\", \"country\": \"United States\"}]\n",
      "\n",
      "From JSON:\n",
      "Canada: Ottawa \"Bytown\"\n",
      "Mexico: Mexico City \"The City of Palaces\"\n",
      "United States: Washington, D.C. \"The District\"\n",
      "\n",
      "LLM Cache: 2 hits, 0 misses\n",
      "           0 new input characters, 0 new output characters, 36 total input characters, 36 total output characters\n",
      "           new (this run) API cost: $0.00, total (including previously-cached runs) API cost: $0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import textwrap\n",
    "\n",
    "import langchain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_google_vertexai.model_garden import ChatAnthropicVertex\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import llm_cache_stats_wrapper\n",
    "import simple_llm_cache\n",
    "\n",
    "# In order to make it easy to run work projects and personal AI experiments, override these key values with the value of *_PERSONAL, if set.\n",
    "if \"OPENAI_API_KEY_PERSONAL\" in os.environ:\n",
    "    print(\"Using key from OPENAI_API_KEY_PERSONAL environment variable\")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.environ[\"OPENAI_API_KEY_PERSONAL\"]\n",
    "if \"ANTHROPIC_API_KEY_PERSONAL\" in os.environ:\n",
    "    print(\"Using key from ANTHROPIC_API_KEY_PERSONAL environment variable\")\n",
    "    os.environ[\"ANTHROPIC_API_KEY\"] = os.environ[\"ANTHROPIC_API_KEY_PERSONAL\"]\n",
    "if \"GOOGLE_APPLICATION_CREDENTIALS_PERSONAL\" in os.environ:\n",
    "    print(\"Using key from GOOGLE_APPLICATION_CREDENTIALS_PERSONAL environment variable\")\n",
    "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = os.environ[\"GOOGLE_APPLICATION_CREDENTIALS_PERSONAL\"]\n",
    "\n",
    "verbose = False\n",
    "temperature = 0.5\n",
    "vertex_model_name_prefix = \"vertexai-\"\n",
    "\n",
    "langchain.llm_cache = llm_cache_stats_wrapper.LlmCacheStatsWrapper(simple_llm_cache.SimpleLlmCache(\"llm-cache.json\"))\n",
    "\n",
    "def dump_cache_stats_since_last_call():\n",
    "    print(langchain.llm_cache.get_cache_stats_summary())\n",
    "    langchain.llm_cache.clear_cache_stats()\n",
    "\n",
    "template = \"\"\"List all the countries in North America and their capital cities. After the name of each city, give it a nickname, placed inside quotation marks. For example, if New York were a capital, you could say New Yord \"The Big Apple\".\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"character\", \"question\"],\n",
    "    template=template)\n",
    "\n",
    "response_schema = {\n",
    "    \"type\": \"array\",\n",
    "    \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"country\": {\n",
    "                \"type\": \"string\",\n",
    "            },\n",
    "            \"capital\": {\n",
    "                \"type\": \"string\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"country\", \"capital\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "for model_name in [\n",
    "    # \"gpt-3.5-turbo\",\n",
    "    # \"gpt-4\",\n",
    "    # \"gpt-4o-mini\",\n",
    "    # \"gpt-4o\",\n",
    "    # \"claude-3-haiku-20240307\",\n",
    "    \"gemini-1.5-pro-001\",\n",
    "    # \"vertexai-claude-3-haiku@20240307\",\n",
    "]:\n",
    "    if model_name.startswith(\"gpt-\"):\n",
    "        llm = ChatOpenAI(\n",
    "            temperature=temperature,\n",
    "            model_name = model_name,\n",
    "            )\n",
    "    elif model_name.startswith(\"claude-\"):\n",
    "        llm = ChatAnthropic(\n",
    "            temperature=temperature,\n",
    "            model_name = model_name)\n",
    "    elif model_name.startswith(\"gemini-\"):\n",
    "        llm = ChatVertexAI(\n",
    "            temperature=temperature,\n",
    "            model_name=model_name,\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=response_schema,\n",
    "        )\n",
    "    elif model_name.startswith(vertex_model_name_prefix):\n",
    "        vertex_model_name = model_name[len(vertex_model_name_prefix) :]\n",
    "        llm = ChatAnthropicVertex(\n",
    "            temperature=temperature, model_name=vertex_model_name\n",
    "        )\n",
    "\n",
    "    chain = LLMChain(\n",
    "        llm=llm,\n",
    "        prompt=prompt,\n",
    "        verbose=verbose)\n",
    "\n",
    "    for trial in range(2):\n",
    "        print(f\"\\n*** {model_name} trial {trial} ***\")\n",
    "        langchain.llm_cache.inner_cache.set_trial(trial)\n",
    "        print(f\"TRIAL {langchain.llm_cache.inner_cache.get_trial()}\")\n",
    "        output = chain.predict()\n",
    "        print(textwrap.fill(output, width=80))\n",
    "        print(\"\\nFrom JSON:\")\n",
    "        json_output = json.loads(output)\n",
    "        for item in json_output:\n",
    "            print(f\"{item['country']}: {item['capital']}\")\n",
    "\n",
    "print()\n",
    "dump_cache_stats_since_last_call()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
